{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Credit / resources\n",
    "- https://github.com/kentaroy47/vision-transformers-cifar10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torchsummary import summary\n",
    "\n",
    "# helpers\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t,t)\n",
    "\n",
    "def posemb_sincos_2d(patches, temperature=10000, dtype=torch.float32):\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "\n",
    "    omega = torch.arange(dim // 4, device=device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature**omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "\n",
    "# Classes\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, 3 * inner_dim, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3,  dim_head = 64, out_sigmoid=True):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.ModuleList([])\n",
    "\n",
    "        if out_sigmoid:\n",
    "            self.linear_head = nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Linear(dim, num_classes),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        else:\n",
    "            self.linear_head = nn.Sequential(\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Linear(dim, num_classes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        *_, h, w, dtype = *img.shape, img.dtype\n",
    "\n",
    "        x = self.to_patch_embedding(img)\n",
    "        pe = posemb_sincos_2d(x)\n",
    "        x = x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1             [-1, 8, 8, 32]               0\n",
      "            Linear-2            [-1, 8, 8, 128]           4,224\n",
      "         LayerNorm-3              [-1, 64, 128]             256\n",
      "            Linear-4              [-1, 64, 768]          98,304\n",
      "           Softmax-5            [-1, 4, 64, 64]               0\n",
      "            Linear-6              [-1, 64, 128]          32,768\n",
      "         Attention-7              [-1, 64, 128]               0\n",
      "         LayerNorm-8              [-1, 64, 128]             256\n",
      "            Linear-9              [-1, 64, 128]          16,512\n",
      "             GELU-10              [-1, 64, 128]               0\n",
      "           Linear-11              [-1, 64, 128]          16,512\n",
      "      FeedForward-12              [-1, 64, 128]               0\n",
      "        LayerNorm-13              [-1, 64, 128]             256\n",
      "           Linear-14              [-1, 64, 768]          98,304\n",
      "          Softmax-15            [-1, 4, 64, 64]               0\n",
      "           Linear-16              [-1, 64, 128]          32,768\n",
      "        Attention-17              [-1, 64, 128]               0\n",
      "        LayerNorm-18              [-1, 64, 128]             256\n",
      "           Linear-19              [-1, 64, 128]          16,512\n",
      "             GELU-20              [-1, 64, 128]               0\n",
      "           Linear-21              [-1, 64, 128]          16,512\n",
      "      FeedForward-22              [-1, 64, 128]               0\n",
      "        LayerNorm-23              [-1, 64, 128]             256\n",
      "           Linear-24              [-1, 64, 768]          98,304\n",
      "          Softmax-25            [-1, 4, 64, 64]               0\n",
      "           Linear-26              [-1, 64, 128]          32,768\n",
      "        Attention-27              [-1, 64, 128]               0\n",
      "        LayerNorm-28              [-1, 64, 128]             256\n",
      "           Linear-29              [-1, 64, 128]          16,512\n",
      "             GELU-30              [-1, 64, 128]               0\n",
      "           Linear-31              [-1, 64, 128]          16,512\n",
      "      FeedForward-32              [-1, 64, 128]               0\n",
      "      Transformer-33              [-1, 64, 128]               0\n",
      "         Identity-34                  [-1, 128]               0\n",
      "        LayerNorm-35                  [-1, 128]             256\n",
      "           Linear-36                    [-1, 1]             129\n",
      "          Sigmoid-37                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 498,433\n",
      "Trainable params: 498,433\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.14\n",
      "Params size (MB): 1.90\n",
      "Estimated Total Size (MB): 5.05\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0.3764],\n        [0.3764]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "v = SimpleViT(image_size=32, patch_size=4, num_classes=1, dim=128, depth=3, heads=4, mlp_dim=128, channels=2,\n",
    "              out_sigmoid=True)\n",
    "v = v.to(device)\n",
    "\n",
    "print(summary(v, (2, 32, 32)))\n",
    "v(torch.zeros(2, 2, 32, 32).to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat, einsum\n",
    "import numpy as np\n",
    "\n",
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        nw_h = n_h // self.window_size\n",
    "        nw_w = n_w // self.window_size\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.stage1(img)\n",
    "        print(x.shape)\n",
    "        x = self.stage2(x)\n",
    "        print(x.shape)\n",
    "        x = self.stage3(x)\n",
    "        print(x.shape)\n",
    "        x = self.stage4(x)\n",
    "        print(x.shape)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        return self.mlp_head(x)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d\".\n Input tensor shape: torch.Size([1, 8, 8, 96]). Additional info: {'h': 3, 'w_h': 7, 'w_w': 7}.\n Shape mismatch, can't divide axis of length 8 in chunks of 7",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    411\u001B[0m         \u001B[0mrecipe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_prepare_transformation_recipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpattern\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxes_lengths\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhashable_axes_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 412\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_apply_recipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecipe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mreduction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    413\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mEinopsError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py\u001B[0m in \u001B[0;36m_apply_recipe\u001B[0;34m(recipe, tensor, reduction_type)\u001B[0m\n\u001B[1;32m    234\u001B[0m     \u001B[0minit_shapes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduced_axes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxes_reordering\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madded_axes\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfinal_shapes\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 235\u001B[0;31m         \u001B[0m_reconstruct_from_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecipe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    236\u001B[0m     \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbackend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minit_shapes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py\u001B[0m in \u001B[0;36m_reconstruct_from_shape_uncached\u001B[0;34m(self, shape)\u001B[0m\n\u001B[1;32m    199\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlength\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mknown_product\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlength\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mknown_product\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m                     raise EinopsError(\"Shape mismatch, can't divide axis of length {} in chunks of {}\".format(\n\u001B[0m\u001B[1;32m    201\u001B[0m                         length, known_product))\n",
      "\u001B[0;31mEinopsError\u001B[0m: Shape mismatch, can't divide axis of length 8 in chunks of 7",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mEinopsError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_22105/775691553.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mswin_t\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 216\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstage1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    217\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstage2\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    185\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpatch_partition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mregular_block\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshifted_block\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mregular_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    188\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mshifted_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mattention_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmlp_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    105\u001B[0m         \u001B[0mnw_w\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mn_w\u001B[0m \u001B[0;34m//\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwindow_size\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 107\u001B[0;31m         q, k, v = map(\n\u001B[0m\u001B[1;32m    108\u001B[0m             lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n\u001B[1;32m    109\u001B[0m                                 h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m         q, k, v = map(\n\u001B[0;32m--> 108\u001B[0;31m             lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n\u001B[0m\u001B[1;32m    109\u001B[0m                                 h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py\u001B[0m in \u001B[0;36mrearrange\u001B[0;34m(tensor, pattern, **axes_lengths)\u001B[0m\n\u001B[1;32m    481\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Rearrange can't be applied to an empty list\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    482\u001B[0m         \u001B[0mtensor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstack_on_zeroth_dimension\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 483\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpattern\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'rearrange'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0maxes_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    484\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(tensor, pattern, reduction, **axes_lengths)\u001B[0m\n\u001B[1;32m    418\u001B[0m             \u001B[0mmessage\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m'\\n Input is list. '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    419\u001B[0m         \u001B[0mmessage\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;34m'Additional info: {}.'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxes_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 420\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mEinopsError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'\\n {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    421\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mEinopsError\u001B[0m:  Error while processing rearrange-reduction pattern \"b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d\".\n Input tensor shape: torch.Size([1, 8, 8, 96]). Additional info: {'h': 3, 'w_h': 7, 'w_w': 7}.\n Shape mismatch, can't divide axis of length 8 in chunks of 7"
     ]
    }
   ],
   "source": [
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "v = swin_t()\n",
    "v = v.to(device)\n",
    "v(torch.zeros(1, 3, 32, 32).to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16, 64])\n",
      "SwinBlock(\n",
      "  (attention_block): Residual(\n",
      "    (fn): PreNorm(\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fn): WindowAttention(\n",
      "        (to_qkv): Linear(in_features=64, out_features=1536, bias=False)\n",
      "        (to_out): Linear(in_features=512, out_features=64, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_block): Residual(\n",
      "    (fn): PreNorm(\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fn): FeedForward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The last argument passed to `einops.einsum` must be a string, representing the einsum pattern.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_22105/3065987417.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m                     relative_pos_embedding=True)\n\u001B[1;32m     39\u001B[0m \u001B[0mstage1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 40\u001B[0;31m \u001B[0mstage1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/3065987417.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mregular_block\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshifted_block\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mregular_block\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mregular_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mshifted_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 146\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mattention_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    147\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmlp_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    148\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1192\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1195\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1196\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_22105/2419239580.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    109\u001B[0m                                 h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n\u001B[1;32m    110\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m         \u001B[0mdots\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meinsum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'b h w i d, b h w j d -> b h w i j'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelative_pos_embedding\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.10/site-packages/einops/einops.py\u001B[0m in \u001B[0;36meinsum\u001B[0;34m(*tensors_and_pattern)\u001B[0m\n\u001B[1;32m    785\u001B[0m     \u001B[0mpattern\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensors_and_pattern\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    786\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpattern\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 787\u001B[0;31m         raise ValueError(\n\u001B[0m\u001B[1;32m    788\u001B[0m             \u001B[0;34m\"The last argument passed to `einops.einsum` must be a string,\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    789\u001B[0m             \u001B[0;34m\" representing the einsum pattern.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: The last argument passed to `einops.einsum` must be a string, representing the einsum pattern."
     ]
    }
   ],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        print(x.shape)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            print(regular_block)\n",
    "            x = regular_block(x)\n",
    "            print(x.shape)\n",
    "            x = shifted_block(x)\n",
    "            print(x.shape)\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "stage1 = StageModule(in_channels=2,\n",
    "                     hidden_dimension=64,\n",
    "                     layers=2,\n",
    "                     downscaling_factor=2,\n",
    "                     num_heads=4,\n",
    "                     head_dim=128,\n",
    "                    window_size=4,\n",
    "                    relative_pos_embedding=True)\n",
    "stage1 = stage1.to(device)\n",
    "stage1(torch.zeros(1, 2, 32, 32).to(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cait"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "\n",
    "from random import randrange\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def dropout_layers(layers, dropout):\n",
    "    if dropout == 0:\n",
    "        return layers\n",
    "\n",
    "    num_layers = len(layers)\n",
    "    to_drop = torch.zeros(num_layers).uniform_(0., 1.) < dropout\n",
    "\n",
    "    # make sure at least one layer makes it\n",
    "    if all(to_drop):\n",
    "        rand_index = randrange(num_layers)\n",
    "        to_drop[rand_index] = False\n",
    "\n",
    "    layers = [layer for (layer, drop) in zip(layers, to_drop) if not drop]\n",
    "    return layers\n",
    "\n",
    "# classes\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, fn, depth):\n",
    "        super().__init__()\n",
    "        if depth <= 18:  # epsilon detailed in section 2 of paper\n",
    "            init_eps = 0.1\n",
    "        elif depth > 18 and depth <= 24:\n",
    "            init_eps = 1e-5\n",
    "        else:\n",
    "            init_eps = 1e-6\n",
    "\n",
    "        scale = torch.zeros(1, 1, dim).fill_(init_eps)\n",
    "        self.scale = nn.Parameter(scale)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.scale\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.mix_heads_pre_attn = nn.Parameter(torch.randn(heads, heads))\n",
    "        self.mix_heads_post_attn = nn.Parameter(torch.randn(heads, heads))\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        context = x if not exists(context) else torch.cat((x, context), dim = 1)\n",
    "\n",
    "        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        dots = einsum('b h i j, h g -> b g i j', dots, self.mix_heads_pre_attn)    # talking heads, pre-softmax\n",
    "        attn = self.attend(dots)\n",
    "        attn = einsum('b h i j, h g -> b g i j', attn, self.mix_heads_post_attn)   # talking heads, post-softmax\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., layer_dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layer_dropout = layer_dropout\n",
    "\n",
    "        for ind in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                LayerScale(dim, PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)), depth = ind + 1),\n",
    "                LayerScale(dim, PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)), depth = ind + 1)\n",
    "            ]))\n",
    "    def forward(self, x, context = None):\n",
    "        layers = dropout_layers(self.layers, dropout = self.layer_dropout)\n",
    "\n",
    "        for attn, ff in layers:\n",
    "            x = attn(x, context = context) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class CaiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        depth,\n",
    "        cls_depth,\n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        emb_dropout = 0.,\n",
    "        layer_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 2 * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.patch_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout, layer_dropout)\n",
    "        self.cls_transformer = Transformer(dim, cls_depth, heads, dim_head, mlp_dim, dropout, layer_dropout)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        x += self.pos_embedding[:, :n]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.patch_transformer(x)\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = self.cls_transformer(cls_tokens, context = x)\n",
    "\n",
    "        return self.mlp_head(x[:, 0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 64, 32]               0\n",
      "            Linear-2                [-1, 64, 4]             132\n",
      "           Dropout-3                [-1, 64, 4]               0\n",
      "         LayerNorm-4                [-1, 64, 4]               8\n",
      "            Linear-5              [-1, 64, 512]           2,048\n",
      "            Linear-6             [-1, 64, 1024]           4,096\n",
      "           Softmax-7            [-1, 8, 64, 64]               0\n",
      "            Linear-8                [-1, 64, 4]           2,052\n",
      "           Dropout-9                [-1, 64, 4]               0\n",
      "        Attention-10                [-1, 64, 4]               0\n",
      "          PreNorm-11                [-1, 64, 4]               0\n",
      "       LayerScale-12                [-1, 64, 4]               0\n",
      "        LayerNorm-13                [-1, 64, 4]               8\n",
      "           Linear-14              [-1, 64, 512]           2,560\n",
      "             GELU-15              [-1, 64, 512]               0\n",
      "          Dropout-16              [-1, 64, 512]               0\n",
      "           Linear-17                [-1, 64, 4]           2,052\n",
      "          Dropout-18                [-1, 64, 4]               0\n",
      "      FeedForward-19                [-1, 64, 4]               0\n",
      "          PreNorm-20                [-1, 64, 4]               0\n",
      "       LayerScale-21                [-1, 64, 4]               0\n",
      "        LayerNorm-22                [-1, 64, 4]               8\n",
      "           Linear-23              [-1, 64, 512]           2,048\n",
      "           Linear-24             [-1, 64, 1024]           4,096\n",
      "          Softmax-25            [-1, 8, 64, 64]               0\n",
      "           Linear-26                [-1, 64, 4]           2,052\n",
      "          Dropout-27                [-1, 64, 4]               0\n",
      "        Attention-28                [-1, 64, 4]               0\n",
      "          PreNorm-29                [-1, 64, 4]               0\n",
      "       LayerScale-30                [-1, 64, 4]               0\n",
      "        LayerNorm-31                [-1, 64, 4]               8\n",
      "           Linear-32              [-1, 64, 512]           2,560\n",
      "             GELU-33              [-1, 64, 512]               0\n",
      "          Dropout-34              [-1, 64, 512]               0\n",
      "           Linear-35                [-1, 64, 4]           2,052\n",
      "          Dropout-36                [-1, 64, 4]               0\n",
      "      FeedForward-37                [-1, 64, 4]               0\n",
      "          PreNorm-38                [-1, 64, 4]               0\n",
      "       LayerScale-39                [-1, 64, 4]               0\n",
      "        LayerNorm-40                [-1, 64, 4]               8\n",
      "           Linear-41              [-1, 64, 512]           2,048\n",
      "           Linear-42             [-1, 64, 1024]           4,096\n",
      "          Softmax-43            [-1, 8, 64, 64]               0\n",
      "           Linear-44                [-1, 64, 4]           2,052\n",
      "          Dropout-45                [-1, 64, 4]               0\n",
      "        Attention-46                [-1, 64, 4]               0\n",
      "          PreNorm-47                [-1, 64, 4]               0\n",
      "       LayerScale-48                [-1, 64, 4]               0\n",
      "        LayerNorm-49                [-1, 64, 4]               8\n",
      "           Linear-50              [-1, 64, 512]           2,560\n",
      "             GELU-51              [-1, 64, 512]               0\n",
      "          Dropout-52              [-1, 64, 512]               0\n",
      "           Linear-53                [-1, 64, 4]           2,052\n",
      "          Dropout-54                [-1, 64, 4]               0\n",
      "      FeedForward-55                [-1, 64, 4]               0\n",
      "          PreNorm-56                [-1, 64, 4]               0\n",
      "       LayerScale-57                [-1, 64, 4]               0\n",
      "        LayerNorm-58                [-1, 64, 4]               8\n",
      "           Linear-59              [-1, 64, 512]           2,048\n",
      "           Linear-60             [-1, 64, 1024]           4,096\n",
      "          Softmax-61            [-1, 8, 64, 64]               0\n",
      "           Linear-62                [-1, 64, 4]           2,052\n",
      "          Dropout-63                [-1, 64, 4]               0\n",
      "        Attention-64                [-1, 64, 4]               0\n",
      "          PreNorm-65                [-1, 64, 4]               0\n",
      "       LayerScale-66                [-1, 64, 4]               0\n",
      "        LayerNorm-67                [-1, 64, 4]               8\n",
      "           Linear-68              [-1, 64, 512]           2,560\n",
      "             GELU-69              [-1, 64, 512]               0\n",
      "          Dropout-70              [-1, 64, 512]               0\n",
      "           Linear-71                [-1, 64, 4]           2,052\n",
      "          Dropout-72                [-1, 64, 4]               0\n",
      "      FeedForward-73                [-1, 64, 4]               0\n",
      "          PreNorm-74                [-1, 64, 4]               0\n",
      "       LayerScale-75                [-1, 64, 4]               0\n",
      "        LayerNorm-76                [-1, 64, 4]               8\n",
      "           Linear-77              [-1, 64, 512]           2,048\n",
      "           Linear-78             [-1, 64, 1024]           4,096\n",
      "          Softmax-79            [-1, 8, 64, 64]               0\n",
      "           Linear-80                [-1, 64, 4]           2,052\n",
      "          Dropout-81                [-1, 64, 4]               0\n",
      "        Attention-82                [-1, 64, 4]               0\n",
      "          PreNorm-83                [-1, 64, 4]               0\n",
      "       LayerScale-84                [-1, 64, 4]               0\n",
      "        LayerNorm-85                [-1, 64, 4]               8\n",
      "           Linear-86              [-1, 64, 512]           2,560\n",
      "             GELU-87              [-1, 64, 512]               0\n",
      "          Dropout-88              [-1, 64, 512]               0\n",
      "           Linear-89                [-1, 64, 4]           2,052\n",
      "          Dropout-90                [-1, 64, 4]               0\n",
      "      FeedForward-91                [-1, 64, 4]               0\n",
      "          PreNorm-92                [-1, 64, 4]               0\n",
      "       LayerScale-93                [-1, 64, 4]               0\n",
      "        LayerNorm-94                [-1, 64, 4]               8\n",
      "           Linear-95              [-1, 64, 512]           2,048\n",
      "           Linear-96             [-1, 64, 1024]           4,096\n",
      "          Softmax-97            [-1, 8, 64, 64]               0\n",
      "           Linear-98                [-1, 64, 4]           2,052\n",
      "          Dropout-99                [-1, 64, 4]               0\n",
      "       Attention-100                [-1, 64, 4]               0\n",
      "         PreNorm-101                [-1, 64, 4]               0\n",
      "      LayerScale-102                [-1, 64, 4]               0\n",
      "       LayerNorm-103                [-1, 64, 4]               8\n",
      "          Linear-104              [-1, 64, 512]           2,560\n",
      "            GELU-105              [-1, 64, 512]               0\n",
      "         Dropout-106              [-1, 64, 512]               0\n",
      "          Linear-107                [-1, 64, 4]           2,052\n",
      "         Dropout-108                [-1, 64, 4]               0\n",
      "     FeedForward-109                [-1, 64, 4]               0\n",
      "         PreNorm-110                [-1, 64, 4]               0\n",
      "      LayerScale-111                [-1, 64, 4]               0\n",
      "     Transformer-112                [-1, 64, 4]               0\n",
      "       LayerNorm-113                 [-1, 1, 4]               8\n",
      "          Linear-114               [-1, 1, 512]           2,048\n",
      "          Linear-115             [-1, 65, 1024]           4,096\n",
      "         Softmax-116             [-1, 8, 1, 65]               0\n",
      "          Linear-117                 [-1, 1, 4]           2,052\n",
      "         Dropout-118                 [-1, 1, 4]               0\n",
      "       Attention-119                 [-1, 1, 4]               0\n",
      "         PreNorm-120                 [-1, 1, 4]               0\n",
      "      LayerScale-121                 [-1, 1, 4]               0\n",
      "       LayerNorm-122                 [-1, 1, 4]               8\n",
      "          Linear-123               [-1, 1, 512]           2,560\n",
      "            GELU-124               [-1, 1, 512]               0\n",
      "         Dropout-125               [-1, 1, 512]               0\n",
      "          Linear-126                 [-1, 1, 4]           2,052\n",
      "         Dropout-127                 [-1, 1, 4]               0\n",
      "     FeedForward-128                 [-1, 1, 4]               0\n",
      "         PreNorm-129                 [-1, 1, 4]               0\n",
      "      LayerScale-130                 [-1, 1, 4]               0\n",
      "       LayerNorm-131                 [-1, 1, 4]               8\n",
      "          Linear-132               [-1, 1, 512]           2,048\n",
      "          Linear-133             [-1, 65, 1024]           4,096\n",
      "         Softmax-134             [-1, 8, 1, 65]               0\n",
      "          Linear-135                 [-1, 1, 4]           2,052\n",
      "         Dropout-136                 [-1, 1, 4]               0\n",
      "       Attention-137                 [-1, 1, 4]               0\n",
      "         PreNorm-138                 [-1, 1, 4]               0\n",
      "      LayerScale-139                 [-1, 1, 4]               0\n",
      "       LayerNorm-140                 [-1, 1, 4]               8\n",
      "          Linear-141               [-1, 1, 512]           2,560\n",
      "            GELU-142               [-1, 1, 512]               0\n",
      "         Dropout-143               [-1, 1, 512]               0\n",
      "          Linear-144                 [-1, 1, 4]           2,052\n",
      "         Dropout-145                 [-1, 1, 4]               0\n",
      "     FeedForward-146                 [-1, 1, 4]               0\n",
      "         PreNorm-147                 [-1, 1, 4]               0\n",
      "      LayerScale-148                 [-1, 1, 4]               0\n",
      "     Transformer-149                 [-1, 1, 4]               0\n",
      "       LayerNorm-150                    [-1, 4]               8\n",
      "          Linear-151                    [-1, 1]               5\n",
      "================================================================\n",
      "Total params: 102,737\n",
      "Trainable params: 102,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 11.72\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 12.12\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_ft = CaiT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 1,\n",
    "    dim = 4,\n",
    "    depth = 6,   # depth of transformer for patch to patch attention only\n",
    "    cls_depth=2, # depth of cross attention of CLS tokens to patch\n",
    "    heads = 8,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    layer_dropout = 0.05\n",
    ")\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "print(summary(model_ft, input_size=(2, 32, 32)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 768, 4, 4]          99,072\n",
      "          Identity-2              [-1, 16, 768]               0\n",
      "        PatchEmbed-3              [-1, 16, 768]               0\n",
      "           Dropout-4              [-1, 16, 768]               0\n",
      "         LayerNorm-5              [-1, 16, 768]           1,536\n",
      "            Linear-6             [-1, 16, 2304]       1,771,776\n",
      "            Linear-7            [-1, 16, 16, 4]              20\n",
      "            Linear-8            [-1, 16, 16, 4]              20\n",
      "           Dropout-9            [-1, 4, 16, 16]               0\n",
      "           Linear-10              [-1, 16, 768]         590,592\n",
      "          Dropout-11              [-1, 16, 768]               0\n",
      "  TalkingHeadAttn-12              [-1, 16, 768]               0\n",
      "         Identity-13              [-1, 16, 768]               0\n",
      "        LayerNorm-14              [-1, 16, 768]           1,536\n",
      "           Linear-15             [-1, 16, 3072]       2,362,368\n",
      "             GELU-16             [-1, 16, 3072]               0\n",
      "          Dropout-17             [-1, 16, 3072]               0\n",
      "           Linear-18              [-1, 16, 768]       2,360,064\n",
      "          Dropout-19              [-1, 16, 768]               0\n",
      "              Mlp-20              [-1, 16, 768]               0\n",
      "         Identity-21              [-1, 16, 768]               0\n",
      "  LayerScaleBlock-22              [-1, 16, 768]               0\n",
      "        LayerNorm-23              [-1, 16, 768]           1,536\n",
      "           Linear-24             [-1, 16, 2304]       1,771,776\n",
      "           Linear-25            [-1, 16, 16, 4]              20\n",
      "           Linear-26            [-1, 16, 16, 4]              20\n",
      "          Dropout-27            [-1, 4, 16, 16]               0\n",
      "           Linear-28              [-1, 16, 768]         590,592\n",
      "          Dropout-29              [-1, 16, 768]               0\n",
      "  TalkingHeadAttn-30              [-1, 16, 768]               0\n",
      "         Identity-31              [-1, 16, 768]               0\n",
      "        LayerNorm-32              [-1, 16, 768]           1,536\n",
      "           Linear-33             [-1, 16, 3072]       2,362,368\n",
      "             GELU-34             [-1, 16, 3072]               0\n",
      "          Dropout-35             [-1, 16, 3072]               0\n",
      "           Linear-36              [-1, 16, 768]       2,360,064\n",
      "          Dropout-37              [-1, 16, 768]               0\n",
      "              Mlp-38              [-1, 16, 768]               0\n",
      "         Identity-39              [-1, 16, 768]               0\n",
      "  LayerScaleBlock-40              [-1, 16, 768]               0\n",
      "        LayerNorm-41              [-1, 17, 768]           1,536\n",
      "           Linear-42                  [-1, 768]         590,592\n",
      "           Linear-43              [-1, 17, 768]         590,592\n",
      "           Linear-44              [-1, 17, 768]         590,592\n",
      "          Dropout-45             [-1, 4, 1, 17]               0\n",
      "           Linear-46               [-1, 1, 768]         590,592\n",
      "          Dropout-47               [-1, 1, 768]               0\n",
      "        ClassAttn-48               [-1, 1, 768]               0\n",
      "         Identity-49               [-1, 1, 768]               0\n",
      "        LayerNorm-50               [-1, 1, 768]           1,536\n",
      "           Linear-51              [-1, 1, 3072]       2,362,368\n",
      "             GELU-52              [-1, 1, 3072]               0\n",
      "          Dropout-53              [-1, 1, 3072]               0\n",
      "           Linear-54               [-1, 1, 768]       2,360,064\n",
      "          Dropout-55               [-1, 1, 768]               0\n",
      "              Mlp-56               [-1, 1, 768]               0\n",
      "         Identity-57               [-1, 1, 768]               0\n",
      "LayerScaleBlockClassAttn-58               [-1, 1, 768]               0\n",
      "        LayerNorm-59              [-1, 17, 768]           1,536\n",
      "           Linear-60                  [-1, 768]         590,592\n",
      "           Linear-61              [-1, 17, 768]         590,592\n",
      "           Linear-62              [-1, 17, 768]         590,592\n",
      "          Dropout-63             [-1, 4, 1, 17]               0\n",
      "           Linear-64               [-1, 1, 768]         590,592\n",
      "          Dropout-65               [-1, 1, 768]               0\n",
      "        ClassAttn-66               [-1, 1, 768]               0\n",
      "         Identity-67               [-1, 1, 768]               0\n",
      "        LayerNorm-68               [-1, 1, 768]           1,536\n",
      "           Linear-69              [-1, 1, 3072]       2,362,368\n",
      "             GELU-70              [-1, 1, 3072]               0\n",
      "          Dropout-71              [-1, 1, 3072]               0\n",
      "           Linear-72               [-1, 1, 768]       2,360,064\n",
      "          Dropout-73               [-1, 1, 768]               0\n",
      "              Mlp-74               [-1, 1, 768]               0\n",
      "         Identity-75               [-1, 1, 768]               0\n",
      "LayerScaleBlockClassAttn-76               [-1, 1, 768]               0\n",
      "        LayerNorm-77              [-1, 17, 768]           1,536\n",
      "           Linear-78                    [-1, 1]             769\n",
      "================================================================\n",
      "Total params: 28,452,945\n",
      "Trainable params: 28,452,945\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 6.26\n",
      "Params size (MB): 108.54\n",
      "Estimated Total Size (MB): 114.81\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from timm.models.cait import Cait\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model_ft = Cait(\n",
    "    img_size = 32,\n",
    "    patch_size = 8,\n",
    "    in_chans = 2,\n",
    "    num_classes = 1,\n",
    "    depth = 2,   # depth of transformer for patch to patch attention only\n",
    "    num_heads = 4,\n",
    "    drop_rate = 0.1\n",
    ")\n",
    "model_ft = model_ft.to(device)\n",
    "print(summary(model_ft, input_size=(2, 32, 32)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from timm.models.mobilevit import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['adv_inception_v3',\n 'bat_resnext26ts',\n 'beit_base_patch16_224',\n 'beit_base_patch16_224_in22k',\n 'beit_base_patch16_384',\n 'beit_large_patch16_224',\n 'beit_large_patch16_224_in22k',\n 'beit_large_patch16_384',\n 'beit_large_patch16_512',\n 'beitv2_base_patch16_224',\n 'beitv2_base_patch16_224_in22k',\n 'beitv2_large_patch16_224',\n 'beitv2_large_patch16_224_in22k',\n 'botnet26t_256',\n 'botnet50ts_256',\n 'cait_m36_384',\n 'cait_m48_448',\n 'cait_s24_224',\n 'cait_s24_384',\n 'cait_s36_384',\n 'cait_xs24_384',\n 'cait_xxs24_224',\n 'cait_xxs24_384',\n 'cait_xxs36_224',\n 'cait_xxs36_384',\n 'coat_lite_mini',\n 'coat_lite_small',\n 'coat_lite_tiny',\n 'coat_mini',\n 'coat_tiny',\n 'coatnet_0_224',\n 'coatnet_0_rw_224',\n 'coatnet_1_224',\n 'coatnet_1_rw_224',\n 'coatnet_2_224',\n 'coatnet_2_rw_224',\n 'coatnet_3_224',\n 'coatnet_3_rw_224',\n 'coatnet_4_224',\n 'coatnet_5_224',\n 'coatnet_bn_0_rw_224',\n 'coatnet_nano_cc_224',\n 'coatnet_nano_rw_224',\n 'coatnet_pico_rw_224',\n 'coatnet_rmlp_0_rw_224',\n 'coatnet_rmlp_1_rw_224',\n 'coatnet_rmlp_2_rw_224',\n 'coatnet_rmlp_3_rw_224',\n 'coatnet_rmlp_nano_rw_224',\n 'coatnext_nano_rw_224',\n 'convit_base',\n 'convit_small',\n 'convit_tiny',\n 'convmixer_768_32',\n 'convmixer_1024_20_ks9_p14',\n 'convmixer_1536_20',\n 'convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_base_384_in22ft1k',\n 'convnext_base_in22ft1k',\n 'convnext_base_in22k',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_384_in22ft1k',\n 'convnext_large_in22ft1k',\n 'convnext_large_in22k',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_small_384_in22ft1k',\n 'convnext_small_in22ft1k',\n 'convnext_small_in22k',\n 'convnext_tiny',\n 'convnext_tiny_384_in22ft1k',\n 'convnext_tiny_hnf',\n 'convnext_tiny_in22ft1k',\n 'convnext_tiny_in22k',\n 'convnext_xlarge_384_in22ft1k',\n 'convnext_xlarge_in22ft1k',\n 'convnext_xlarge_in22k',\n 'crossvit_9_240',\n 'crossvit_9_dagger_240',\n 'crossvit_15_240',\n 'crossvit_15_dagger_240',\n 'crossvit_15_dagger_408',\n 'crossvit_18_240',\n 'crossvit_18_dagger_240',\n 'crossvit_18_dagger_408',\n 'crossvit_base_240',\n 'crossvit_small_240',\n 'crossvit_tiny_240',\n 'cs3darknet_focus_l',\n 'cs3darknet_focus_m',\n 'cs3darknet_focus_s',\n 'cs3darknet_focus_x',\n 'cs3darknet_l',\n 'cs3darknet_m',\n 'cs3darknet_s',\n 'cs3darknet_x',\n 'cs3edgenet_x',\n 'cs3se_edgenet_x',\n 'cs3sedarknet_l',\n 'cs3sedarknet_x',\n 'cs3sedarknet_xdw',\n 'cspdarknet53',\n 'cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'cspresnext50',\n 'darknet17',\n 'darknet21',\n 'darknet53',\n 'darknetaa53',\n 'deit3_base_patch16_224',\n 'deit3_base_patch16_224_in21ft1k',\n 'deit3_base_patch16_384',\n 'deit3_base_patch16_384_in21ft1k',\n 'deit3_huge_patch14_224',\n 'deit3_huge_patch14_224_in21ft1k',\n 'deit3_large_patch16_224',\n 'deit3_large_patch16_224_in21ft1k',\n 'deit3_large_patch16_384',\n 'deit3_large_patch16_384_in21ft1k',\n 'deit3_medium_patch16_224',\n 'deit3_medium_patch16_224_in21ft1k',\n 'deit3_small_patch16_224',\n 'deit3_small_patch16_224_in21ft1k',\n 'deit3_small_patch16_384',\n 'deit3_small_patch16_384_in21ft1k',\n 'deit_base_distilled_patch16_224',\n 'deit_base_distilled_patch16_384',\n 'deit_base_patch16_224',\n 'deit_base_patch16_384',\n 'deit_small_distilled_patch16_224',\n 'deit_small_patch16_224',\n 'deit_tiny_distilled_patch16_224',\n 'deit_tiny_patch16_224',\n 'densenet121',\n 'densenet121d',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenet264',\n 'densenet264d_iabn',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n 'dla46x_c',\n 'dla60',\n 'dla60_res2net',\n 'dla60_res2next',\n 'dla60x',\n 'dla60x_c',\n 'dla102',\n 'dla102x',\n 'dla102x2',\n 'dla169',\n 'dm_nfnet_f0',\n 'dm_nfnet_f1',\n 'dm_nfnet_f2',\n 'dm_nfnet_f3',\n 'dm_nfnet_f4',\n 'dm_nfnet_f5',\n 'dm_nfnet_f6',\n 'dpn68',\n 'dpn68b',\n 'dpn92',\n 'dpn98',\n 'dpn107',\n 'dpn131',\n 'eca_botnext26ts_256',\n 'eca_halonext26ts',\n 'eca_nfnet_l0',\n 'eca_nfnet_l1',\n 'eca_nfnet_l2',\n 'eca_nfnet_l3',\n 'eca_resnet33ts',\n 'eca_resnext26ts',\n 'eca_vovnet39b',\n 'ecaresnet26t',\n 'ecaresnet50d',\n 'ecaresnet50d_pruned',\n 'ecaresnet50t',\n 'ecaresnet101d',\n 'ecaresnet101d_pruned',\n 'ecaresnet200d',\n 'ecaresnet269d',\n 'ecaresnetlight',\n 'ecaresnext26t_32x4d',\n 'ecaresnext50t_32x4d',\n 'edgenext_base',\n 'edgenext_small',\n 'edgenext_small_rw',\n 'edgenext_x_small',\n 'edgenext_xx_small',\n 'efficientformer_l1',\n 'efficientformer_l3',\n 'efficientformer_l7',\n 'efficientnet_b0',\n 'efficientnet_b0_g8_gn',\n 'efficientnet_b0_g16_evos',\n 'efficientnet_b0_gn',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b2a',\n 'efficientnet_b3',\n 'efficientnet_b3_g8_gn',\n 'efficientnet_b3_gn',\n 'efficientnet_b3_pruned',\n 'efficientnet_b3a',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_b8',\n 'efficientnet_cc_b0_4e',\n 'efficientnet_cc_b0_8e',\n 'efficientnet_cc_b1_8e',\n 'efficientnet_el',\n 'efficientnet_el_pruned',\n 'efficientnet_em',\n 'efficientnet_es',\n 'efficientnet_es_pruned',\n 'efficientnet_l2',\n 'efficientnet_lite0',\n 'efficientnet_lite1',\n 'efficientnet_lite2',\n 'efficientnet_lite3',\n 'efficientnet_lite4',\n 'efficientnetv2_l',\n 'efficientnetv2_m',\n 'efficientnetv2_rw_m',\n 'efficientnetv2_rw_s',\n 'efficientnetv2_rw_t',\n 'efficientnetv2_s',\n 'efficientnetv2_xl',\n 'ens_adv_inception_resnet_v2',\n 'ese_vovnet19b_dw',\n 'ese_vovnet19b_slim',\n 'ese_vovnet19b_slim_dw',\n 'ese_vovnet39b',\n 'ese_vovnet39b_evos',\n 'ese_vovnet57b',\n 'ese_vovnet99b',\n 'ese_vovnet99b_iabn',\n 'fbnetc_100',\n 'fbnetv3_b',\n 'fbnetv3_d',\n 'fbnetv3_g',\n 'gc_efficientnetv2_rw_t',\n 'gcresnet33ts',\n 'gcresnet50t',\n 'gcresnext26ts',\n 'gcresnext50ts',\n 'gcvit_base',\n 'gcvit_small',\n 'gcvit_tiny',\n 'gcvit_xtiny',\n 'gcvit_xxtiny',\n 'gernet_l',\n 'gernet_m',\n 'gernet_s',\n 'ghostnet_050',\n 'ghostnet_100',\n 'ghostnet_130',\n 'gluon_inception_v3',\n 'gluon_resnet18_v1b',\n 'gluon_resnet34_v1b',\n 'gluon_resnet50_v1b',\n 'gluon_resnet50_v1c',\n 'gluon_resnet50_v1d',\n 'gluon_resnet50_v1s',\n 'gluon_resnet101_v1b',\n 'gluon_resnet101_v1c',\n 'gluon_resnet101_v1d',\n 'gluon_resnet101_v1s',\n 'gluon_resnet152_v1b',\n 'gluon_resnet152_v1c',\n 'gluon_resnet152_v1d',\n 'gluon_resnet152_v1s',\n 'gluon_resnext50_32x4d',\n 'gluon_resnext101_32x4d',\n 'gluon_resnext101_64x4d',\n 'gluon_senet154',\n 'gluon_seresnext50_32x4d',\n 'gluon_seresnext101_32x4d',\n 'gluon_seresnext101_64x4d',\n 'gluon_xception65',\n 'gmixer_12_224',\n 'gmixer_24_224',\n 'gmlp_b16_224',\n 'gmlp_s16_224',\n 'gmlp_ti16_224',\n 'halo2botnet50ts_256',\n 'halonet26t',\n 'halonet50ts',\n 'halonet_h1',\n 'haloregnetz_b',\n 'hardcorenas_a',\n 'hardcorenas_b',\n 'hardcorenas_c',\n 'hardcorenas_d',\n 'hardcorenas_e',\n 'hardcorenas_f',\n 'hrnet_w18',\n 'hrnet_w18_small',\n 'hrnet_w18_small_v2',\n 'hrnet_w30',\n 'hrnet_w32',\n 'hrnet_w40',\n 'hrnet_w44',\n 'hrnet_w48',\n 'hrnet_w64',\n 'ig_resnext101_32x8d',\n 'ig_resnext101_32x16d',\n 'ig_resnext101_32x32d',\n 'ig_resnext101_32x48d',\n 'inception_resnet_v2',\n 'inception_v3',\n 'inception_v4',\n 'jx_nest_base',\n 'jx_nest_small',\n 'jx_nest_tiny',\n 'lambda_resnet26rpt_256',\n 'lambda_resnet26t',\n 'lambda_resnet50ts',\n 'lamhalobotnet50ts_256',\n 'lcnet_035',\n 'lcnet_050',\n 'lcnet_075',\n 'lcnet_100',\n 'lcnet_150',\n 'legacy_senet154',\n 'legacy_seresnet18',\n 'legacy_seresnet34',\n 'legacy_seresnet50',\n 'legacy_seresnet101',\n 'legacy_seresnet152',\n 'legacy_seresnext26_32x4d',\n 'legacy_seresnext50_32x4d',\n 'legacy_seresnext101_32x4d',\n 'levit_128',\n 'levit_128s',\n 'levit_192',\n 'levit_256',\n 'levit_256d',\n 'levit_384',\n 'maxvit_base_224',\n 'maxvit_large_224',\n 'maxvit_nano_rw_256',\n 'maxvit_pico_rw_256',\n 'maxvit_rmlp_nano_rw_256',\n 'maxvit_rmlp_pico_rw_256',\n 'maxvit_rmlp_small_rw_224',\n 'maxvit_rmlp_small_rw_256',\n 'maxvit_rmlp_tiny_rw_256',\n 'maxvit_small_224',\n 'maxvit_tiny_224',\n 'maxvit_tiny_pm_256',\n 'maxvit_tiny_rw_224',\n 'maxvit_tiny_rw_256',\n 'maxvit_xlarge_224',\n 'maxxvit_rmlp_nano_rw_256',\n 'maxxvit_rmlp_small_rw_256',\n 'maxxvit_rmlp_tiny_rw_256',\n 'mixer_b16_224',\n 'mixer_b16_224_in21k',\n 'mixer_b16_224_miil',\n 'mixer_b16_224_miil_in21k',\n 'mixer_b32_224',\n 'mixer_l16_224',\n 'mixer_l16_224_in21k',\n 'mixer_l32_224',\n 'mixer_s16_224',\n 'mixer_s32_224',\n 'mixnet_l',\n 'mixnet_m',\n 'mixnet_s',\n 'mixnet_xl',\n 'mixnet_xxl',\n 'mnasnet_050',\n 'mnasnet_075',\n 'mnasnet_100',\n 'mnasnet_140',\n 'mnasnet_a1',\n 'mnasnet_b1',\n 'mnasnet_small',\n 'mobilenetv2_035',\n 'mobilenetv2_050',\n 'mobilenetv2_075',\n 'mobilenetv2_100',\n 'mobilenetv2_110d',\n 'mobilenetv2_120d',\n 'mobilenetv2_140',\n 'mobilenetv3_large_075',\n 'mobilenetv3_large_100',\n 'mobilenetv3_large_100_miil',\n 'mobilenetv3_large_100_miil_in21k',\n 'mobilenetv3_rw',\n 'mobilenetv3_small_050',\n 'mobilenetv3_small_075',\n 'mobilenetv3_small_100',\n 'mobilevit_s',\n 'mobilevit_xs',\n 'mobilevit_xxs',\n 'mobilevitv2_050',\n 'mobilevitv2_075',\n 'mobilevitv2_100',\n 'mobilevitv2_125',\n 'mobilevitv2_150',\n 'mobilevitv2_150_384_in22ft1k',\n 'mobilevitv2_150_in22ft1k',\n 'mobilevitv2_175',\n 'mobilevitv2_175_384_in22ft1k',\n 'mobilevitv2_175_in22ft1k',\n 'mobilevitv2_200',\n 'mobilevitv2_200_384_in22ft1k',\n 'mobilevitv2_200_in22ft1k',\n 'mvitv2_base',\n 'mvitv2_large',\n 'mvitv2_small',\n 'mvitv2_small_cls',\n 'mvitv2_tiny',\n 'nasnetalarge',\n 'nest_base',\n 'nest_small',\n 'nest_tiny',\n 'nf_ecaresnet26',\n 'nf_ecaresnet50',\n 'nf_ecaresnet101',\n 'nf_regnet_b0',\n 'nf_regnet_b1',\n 'nf_regnet_b2',\n 'nf_regnet_b3',\n 'nf_regnet_b4',\n 'nf_regnet_b5',\n 'nf_resnet26',\n 'nf_resnet50',\n 'nf_resnet101',\n 'nf_seresnet26',\n 'nf_seresnet50',\n 'nf_seresnet101',\n 'nfnet_f0',\n 'nfnet_f1',\n 'nfnet_f2',\n 'nfnet_f3',\n 'nfnet_f4',\n 'nfnet_f5',\n 'nfnet_f6',\n 'nfnet_f7',\n 'nfnet_l0',\n 'pit_b_224',\n 'pit_b_distilled_224',\n 'pit_s_224',\n 'pit_s_distilled_224',\n 'pit_ti_224',\n 'pit_ti_distilled_224',\n 'pit_xs_224',\n 'pit_xs_distilled_224',\n 'pnasnet5large',\n 'poolformer_m36',\n 'poolformer_m48',\n 'poolformer_s12',\n 'poolformer_s24',\n 'poolformer_s36',\n 'pvt_v2_b0',\n 'pvt_v2_b1',\n 'pvt_v2_b2',\n 'pvt_v2_b2_li',\n 'pvt_v2_b3',\n 'pvt_v2_b4',\n 'pvt_v2_b5',\n 'regnetv_040',\n 'regnetv_064',\n 'regnetx_002',\n 'regnetx_004',\n 'regnetx_006',\n 'regnetx_008',\n 'regnetx_016',\n 'regnetx_032',\n 'regnetx_040',\n 'regnetx_064',\n 'regnetx_080',\n 'regnetx_120',\n 'regnetx_160',\n 'regnetx_320',\n 'regnety_002',\n 'regnety_004',\n 'regnety_006',\n 'regnety_008',\n 'regnety_016',\n 'regnety_032',\n 'regnety_040',\n 'regnety_040s_gn',\n 'regnety_064',\n 'regnety_080',\n 'regnety_120',\n 'regnety_160',\n 'regnety_320',\n 'regnetz_005',\n 'regnetz_040',\n 'regnetz_040h',\n 'regnetz_b16',\n 'regnetz_b16_evos',\n 'regnetz_c16',\n 'regnetz_c16_evos',\n 'regnetz_d8',\n 'regnetz_d8_evos',\n 'regnetz_d32',\n 'regnetz_e8',\n 'repvgg_a2',\n 'repvgg_b0',\n 'repvgg_b1',\n 'repvgg_b1g4',\n 'repvgg_b2',\n 'repvgg_b2g4',\n 'repvgg_b3',\n 'repvgg_b3g4',\n 'res2net50_14w_8s',\n 'res2net50_26w_4s',\n 'res2net50_26w_6s',\n 'res2net50_26w_8s',\n 'res2net50_48w_2s',\n 'res2net101_26w_4s',\n 'res2next50',\n 'resmlp_12_224',\n 'resmlp_12_224_dino',\n 'resmlp_12_distilled_224',\n 'resmlp_24_224',\n 'resmlp_24_224_dino',\n 'resmlp_24_distilled_224',\n 'resmlp_36_224',\n 'resmlp_36_distilled_224',\n 'resmlp_big_24_224',\n 'resmlp_big_24_224_in22ft1k',\n 'resmlp_big_24_distilled_224',\n 'resnest14d',\n 'resnest26d',\n 'resnest50d',\n 'resnest50d_1s4x24d',\n 'resnest50d_4s2x40d',\n 'resnest101e',\n 'resnest200e',\n 'resnest269e',\n 'resnet10t',\n 'resnet14t',\n 'resnet18',\n 'resnet18d',\n 'resnet26',\n 'resnet26d',\n 'resnet26t',\n 'resnet32ts',\n 'resnet33ts',\n 'resnet34',\n 'resnet34d',\n 'resnet50',\n 'resnet50_gn',\n 'resnet50d',\n 'resnet50t',\n 'resnet51q',\n 'resnet61q',\n 'resnet101',\n 'resnet101d',\n 'resnet152',\n 'resnet152d',\n 'resnet200',\n 'resnet200d',\n 'resnetaa50',\n 'resnetaa50d',\n 'resnetaa101d',\n 'resnetblur18',\n 'resnetblur50',\n 'resnetblur50d',\n 'resnetblur101d',\n 'resnetrs50',\n 'resnetrs101',\n 'resnetrs152',\n 'resnetrs200',\n 'resnetrs270',\n 'resnetrs350',\n 'resnetrs420',\n 'resnetv2_50',\n 'resnetv2_50d',\n 'resnetv2_50d_evob',\n 'resnetv2_50d_evos',\n 'resnetv2_50d_frn',\n 'resnetv2_50d_gn',\n 'resnetv2_50t',\n 'resnetv2_50x1_bit_distilled',\n 'resnetv2_50x1_bitm',\n 'resnetv2_50x1_bitm_in21k',\n 'resnetv2_50x3_bitm',\n 'resnetv2_50x3_bitm_in21k',\n 'resnetv2_101',\n 'resnetv2_101d',\n 'resnetv2_101x1_bitm',\n 'resnetv2_101x1_bitm_in21k',\n 'resnetv2_101x3_bitm',\n 'resnetv2_101x3_bitm_in21k',\n 'resnetv2_152',\n 'resnetv2_152d',\n 'resnetv2_152x2_bit_teacher',\n 'resnetv2_152x2_bit_teacher_384',\n 'resnetv2_152x2_bitm',\n 'resnetv2_152x2_bitm_in21k',\n 'resnetv2_152x4_bitm',\n 'resnetv2_152x4_bitm_in21k',\n 'resnext26ts',\n 'resnext50_32x4d',\n 'resnext50d_32x4d',\n 'resnext101_32x4d',\n 'resnext101_32x8d',\n 'resnext101_64x4d',\n 'rexnet_100',\n 'rexnet_130',\n 'rexnet_150',\n 'rexnet_200',\n 'rexnetr_100',\n 'rexnetr_130',\n 'rexnetr_150',\n 'rexnetr_200',\n 'sebotnet33ts_256',\n 'sedarknet21',\n 'sehalonet33ts',\n 'selecsls42',\n 'selecsls42b',\n 'selecsls60',\n 'selecsls60b',\n 'selecsls84',\n 'semnasnet_050',\n 'semnasnet_075',\n 'semnasnet_100',\n 'semnasnet_140',\n 'semobilevit_s',\n 'senet154',\n 'sequencer2d_l',\n 'sequencer2d_m',\n 'sequencer2d_s',\n 'seresnet18',\n 'seresnet33ts',\n 'seresnet34',\n 'seresnet50',\n 'seresnet50t',\n 'seresnet101',\n 'seresnet152',\n 'seresnet152d',\n 'seresnet200d',\n 'seresnet269d',\n 'seresnetaa50d',\n 'seresnext26d_32x4d',\n 'seresnext26t_32x4d',\n 'seresnext26tn_32x4d',\n 'seresnext26ts',\n 'seresnext50_32x4d',\n 'seresnext101_32x4d',\n 'seresnext101_32x8d',\n 'seresnext101d_32x8d',\n 'seresnextaa101d_32x8d',\n 'skresnet18',\n 'skresnet34',\n 'skresnet50',\n 'skresnet50d',\n 'skresnext50_32x4d',\n 'spnasnet_100',\n 'ssl_resnet18',\n 'ssl_resnet50',\n 'ssl_resnext50_32x4d',\n 'ssl_resnext101_32x4d',\n 'ssl_resnext101_32x8d',\n 'ssl_resnext101_32x16d',\n 'swin_base_patch4_window7_224',\n 'swin_base_patch4_window7_224_in22k',\n 'swin_base_patch4_window12_384',\n 'swin_base_patch4_window12_384_in22k',\n 'swin_large_patch4_window7_224',\n 'swin_large_patch4_window7_224_in22k',\n 'swin_large_patch4_window12_384',\n 'swin_large_patch4_window12_384_in22k',\n 'swin_s3_base_224',\n 'swin_s3_small_224',\n 'swin_s3_tiny_224',\n 'swin_small_patch4_window7_224',\n 'swin_tiny_patch4_window7_224',\n 'swinv2_base_window8_256',\n 'swinv2_base_window12_192_22k',\n 'swinv2_base_window12to16_192to256_22kft1k',\n 'swinv2_base_window12to24_192to384_22kft1k',\n 'swinv2_base_window16_256',\n 'swinv2_cr_base_224',\n 'swinv2_cr_base_384',\n 'swinv2_cr_base_ns_224',\n 'swinv2_cr_giant_224',\n 'swinv2_cr_giant_384',\n 'swinv2_cr_huge_224',\n 'swinv2_cr_huge_384',\n 'swinv2_cr_large_224',\n 'swinv2_cr_large_384',\n 'swinv2_cr_small_224',\n 'swinv2_cr_small_384',\n 'swinv2_cr_small_ns_224',\n 'swinv2_cr_tiny_224',\n 'swinv2_cr_tiny_384',\n 'swinv2_cr_tiny_ns_224',\n 'swinv2_large_window12_192_22k',\n 'swinv2_large_window12to16_192to256_22kft1k',\n 'swinv2_large_window12to24_192to384_22kft1k',\n 'swinv2_small_window8_256',\n 'swinv2_small_window16_256',\n 'swinv2_tiny_window8_256',\n 'swinv2_tiny_window16_256',\n 'swsl_resnet18',\n 'swsl_resnet50',\n 'swsl_resnext50_32x4d',\n 'swsl_resnext101_32x4d',\n 'swsl_resnext101_32x8d',\n 'swsl_resnext101_32x16d',\n 'tf_efficientnet_b0',\n 'tf_efficientnet_b0_ap',\n 'tf_efficientnet_b0_ns',\n 'tf_efficientnet_b1',\n 'tf_efficientnet_b1_ap',\n 'tf_efficientnet_b1_ns',\n 'tf_efficientnet_b2',\n 'tf_efficientnet_b2_ap',\n 'tf_efficientnet_b2_ns',\n 'tf_efficientnet_b3',\n 'tf_efficientnet_b3_ap',\n 'tf_efficientnet_b3_ns',\n 'tf_efficientnet_b4',\n 'tf_efficientnet_b4_ap',\n 'tf_efficientnet_b4_ns',\n 'tf_efficientnet_b5',\n 'tf_efficientnet_b5_ap',\n 'tf_efficientnet_b5_ns',\n 'tf_efficientnet_b6',\n 'tf_efficientnet_b6_ap',\n 'tf_efficientnet_b6_ns',\n 'tf_efficientnet_b7',\n 'tf_efficientnet_b7_ap',\n 'tf_efficientnet_b7_ns',\n 'tf_efficientnet_b8',\n 'tf_efficientnet_b8_ap',\n 'tf_efficientnet_cc_b0_4e',\n 'tf_efficientnet_cc_b0_8e',\n 'tf_efficientnet_cc_b1_8e',\n 'tf_efficientnet_el',\n 'tf_efficientnet_em',\n 'tf_efficientnet_es',\n 'tf_efficientnet_l2_ns',\n 'tf_efficientnet_l2_ns_475',\n 'tf_efficientnet_lite0',\n 'tf_efficientnet_lite1',\n 'tf_efficientnet_lite2',\n 'tf_efficientnet_lite3',\n 'tf_efficientnet_lite4',\n 'tf_efficientnetv2_b0',\n 'tf_efficientnetv2_b1',\n 'tf_efficientnetv2_b2',\n 'tf_efficientnetv2_b3',\n 'tf_efficientnetv2_l',\n 'tf_efficientnetv2_l_in21ft1k',\n 'tf_efficientnetv2_l_in21k',\n 'tf_efficientnetv2_m',\n 'tf_efficientnetv2_m_in21ft1k',\n 'tf_efficientnetv2_m_in21k',\n 'tf_efficientnetv2_s',\n 'tf_efficientnetv2_s_in21ft1k',\n 'tf_efficientnetv2_s_in21k',\n 'tf_efficientnetv2_xl_in21ft1k',\n 'tf_efficientnetv2_xl_in21k',\n 'tf_inception_v3',\n 'tf_mixnet_l',\n 'tf_mixnet_m',\n 'tf_mixnet_s',\n 'tf_mobilenetv3_large_075',\n 'tf_mobilenetv3_large_100',\n 'tf_mobilenetv3_large_minimal_100',\n 'tf_mobilenetv3_small_075',\n 'tf_mobilenetv3_small_100',\n 'tf_mobilenetv3_small_minimal_100',\n 'tinynet_a',\n 'tinynet_b',\n 'tinynet_c',\n 'tinynet_d',\n 'tinynet_e',\n 'tnt_b_patch16_224',\n 'tnt_s_patch16_224',\n 'tresnet_l',\n 'tresnet_l_448',\n 'tresnet_m',\n 'tresnet_m_448',\n 'tresnet_m_miil_in21k',\n 'tresnet_v2_l',\n 'tresnet_xl',\n 'tresnet_xl_448',\n 'tv_densenet121',\n 'tv_resnet34',\n 'tv_resnet50',\n 'tv_resnet101',\n 'tv_resnet152',\n 'tv_resnext50_32x4d',\n 'twins_pcpvt_base',\n 'twins_pcpvt_large',\n 'twins_pcpvt_small',\n 'twins_svt_base',\n 'twins_svt_large',\n 'twins_svt_small',\n 'vgg11',\n 'vgg11_bn',\n 'vgg13',\n 'vgg13_bn',\n 'vgg16',\n 'vgg16_bn',\n 'vgg19',\n 'vgg19_bn',\n 'visformer_small',\n 'visformer_tiny',\n 'vit_base_patch8_224',\n 'vit_base_patch8_224_dino',\n 'vit_base_patch8_224_in21k',\n 'vit_base_patch16_18x2_224',\n 'vit_base_patch16_224',\n 'vit_base_patch16_224_dino',\n 'vit_base_patch16_224_in21k',\n 'vit_base_patch16_224_miil',\n 'vit_base_patch16_224_miil_in21k',\n 'vit_base_patch16_224_sam',\n 'vit_base_patch16_384',\n 'vit_base_patch16_plus_240',\n 'vit_base_patch16_rpn_224',\n 'vit_base_patch32_224',\n 'vit_base_patch32_224_clip_laion2b',\n 'vit_base_patch32_224_in21k',\n 'vit_base_patch32_224_sam',\n 'vit_base_patch32_384',\n 'vit_base_patch32_plus_256',\n 'vit_base_r26_s32_224',\n 'vit_base_r50_s16_224',\n 'vit_base_r50_s16_224_in21k',\n 'vit_base_r50_s16_384',\n 'vit_base_resnet26d_224',\n 'vit_base_resnet50_224_in21k',\n 'vit_base_resnet50_384',\n 'vit_base_resnet50d_224',\n 'vit_giant_patch14_224',\n 'vit_giant_patch14_224_clip_laion2b',\n 'vit_gigantic_patch14_224',\n 'vit_huge_patch14_224',\n 'vit_huge_patch14_224_clip_laion2b',\n 'vit_huge_patch14_224_in21k',\n 'vit_large_patch14_224',\n 'vit_large_patch14_224_clip_laion2b',\n 'vit_large_patch16_224',\n 'vit_large_patch16_224_in21k',\n 'vit_large_patch16_384',\n 'vit_large_patch32_224',\n 'vit_large_patch32_224_in21k',\n 'vit_large_patch32_384',\n 'vit_large_r50_s32_224',\n 'vit_large_r50_s32_224_in21k',\n 'vit_large_r50_s32_384',\n 'vit_relpos_base_patch16_224',\n 'vit_relpos_base_patch16_cls_224',\n 'vit_relpos_base_patch16_clsgap_224',\n 'vit_relpos_base_patch16_plus_240',\n 'vit_relpos_base_patch16_rpn_224',\n 'vit_relpos_base_patch32_plus_rpn_256',\n 'vit_relpos_medium_patch16_224',\n 'vit_relpos_medium_patch16_cls_224',\n 'vit_relpos_medium_patch16_rpn_224',\n 'vit_relpos_small_patch16_224',\n 'vit_relpos_small_patch16_rpn_224',\n 'vit_small_patch8_224_dino',\n 'vit_small_patch16_18x2_224',\n 'vit_small_patch16_36x1_224',\n 'vit_small_patch16_224',\n 'vit_small_patch16_224_dino',\n 'vit_small_patch16_224_in21k',\n 'vit_small_patch16_384',\n 'vit_small_patch32_224',\n 'vit_small_patch32_224_in21k',\n 'vit_small_patch32_384',\n 'vit_small_r26_s32_224',\n 'vit_small_r26_s32_224_in21k',\n 'vit_small_r26_s32_384',\n 'vit_small_resnet26d_224',\n 'vit_small_resnet50d_s16_224',\n 'vit_srelpos_medium_patch16_224',\n 'vit_srelpos_small_patch16_224',\n 'vit_tiny_patch16_224',\n 'vit_tiny_patch16_224_in21k',\n 'vit_tiny_patch16_384',\n 'vit_tiny_r_s16_p8_224',\n 'vit_tiny_r_s16_p8_224_in21k',\n 'vit_tiny_r_s16_p8_384',\n 'volo_d1_224',\n 'volo_d1_384',\n 'volo_d2_224',\n 'volo_d2_384',\n 'volo_d3_224',\n 'volo_d3_448',\n 'volo_d4_224',\n 'volo_d4_448',\n 'volo_d5_224',\n 'volo_d5_448',\n 'volo_d5_512',\n 'vovnet39a',\n 'vovnet57a',\n 'wide_resnet50_2',\n 'wide_resnet101_2',\n 'xception',\n 'xception41',\n 'xception41p',\n 'xception65',\n 'xception65p',\n 'xception71',\n 'xcit_large_24_p8_224',\n 'xcit_large_24_p8_224_dist',\n 'xcit_large_24_p8_384_dist',\n 'xcit_large_24_p16_224',\n 'xcit_large_24_p16_224_dist',\n 'xcit_large_24_p16_384_dist',\n 'xcit_medium_24_p8_224',\n 'xcit_medium_24_p8_224_dist',\n 'xcit_medium_24_p8_384_dist',\n 'xcit_medium_24_p16_224',\n 'xcit_medium_24_p16_224_dist',\n 'xcit_medium_24_p16_384_dist',\n 'xcit_nano_12_p8_224',\n 'xcit_nano_12_p8_224_dist',\n 'xcit_nano_12_p8_384_dist',\n 'xcit_nano_12_p16_224',\n 'xcit_nano_12_p16_224_dist',\n 'xcit_nano_12_p16_384_dist',\n 'xcit_small_12_p8_224',\n 'xcit_small_12_p8_224_dist',\n 'xcit_small_12_p8_384_dist',\n 'xcit_small_12_p16_224',\n 'xcit_small_12_p16_224_dist',\n 'xcit_small_12_p16_384_dist',\n 'xcit_small_24_p8_224',\n 'xcit_small_24_p8_224_dist',\n 'xcit_small_24_p8_384_dist',\n 'xcit_small_24_p16_224',\n 'xcit_small_24_p16_224_dist',\n 'xcit_small_24_p16_384_dist',\n 'xcit_tiny_12_p8_224',\n 'xcit_tiny_12_p8_224_dist',\n 'xcit_tiny_12_p8_384_dist',\n 'xcit_tiny_12_p16_224',\n 'xcit_tiny_12_p16_224_dist',\n 'xcit_tiny_12_p16_384_dist',\n 'xcit_tiny_24_p8_224',\n 'xcit_tiny_24_p8_224_dist',\n 'xcit_tiny_24_p8_384_dist',\n 'xcit_tiny_24_p16_224',\n 'xcit_tiny_24_p16_224_dist',\n 'xcit_tiny_24_p16_384_dist']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "timm.list_models()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
